{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1063155-11db-4b7f-bd84-9027e13c0366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Define the transform to apply to the data\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "test = torchvision.datasets.ImageFolder(\n",
    "    root='images/test',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "train = torchvision.datasets.ImageFolder(\n",
    "    root='images/train',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "validation = torchvision.datasets.ImageFolder(\n",
    "    root='images/validation',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe52f75-a2ad-4191-b68f-730e728ba547",
   "metadata": {},
   "source": [
    "visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea171914-1776-4cd5-90b2-55d94d3b3fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape:  torch.Size([64, 3, 224, 224])\n",
      "Labels shape:  torch.Size([64])\n",
      "Mean=0.43442869186401367, Std=0.2507145404815674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 223.5, 223.5, -0.5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterr = iter(train_loader)\n",
    "images, labels = next(iterr)\n",
    "\n",
    "# Print information and statistics of the first batch of images\n",
    "print(\"Images shape: \", images.shape)\n",
    "print(\"Labels shape: \", labels.shape)\n",
    "print(f'Mean={images.mean()}, Std={images.std()}')\n",
    "\n",
    "i = 2\n",
    "plt.subplot(4, 5, i+1)\n",
    "plt.imshow(images[i].permute(1, 2, 0), cmap='gray', interpolation='none')\n",
    "plt.title(f'Label: {labels[i]}', fontsize=14)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a5423a-f829-4d06-9c0f-b316a00155ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 1 * 224 * 224  # input spatial dimension of images\n",
    "hidden_size = 128         # width of hidden layer\n",
    "output_size = 15          # number of output neurons\n",
    "\n",
    "\n",
    "class MyCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.features = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512 * 7 * 7, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Dropout(),\n",
    "            torch.nn.Linear(4096, 15),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 512 * 7 * 7)\n",
    "        x = self.classifier(x)\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# Create an instance of the model and move it to the GPU if available\n",
    "model = MyCNN().to(DEVICE)\n",
    "\n",
    "    \n",
    "\n",
    "# sanity check\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb76203-b85d-4893-8613-70cbd3ed986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, device, optimizer, log_interval, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    losses = []\n",
    "    counter = []\n",
    "    \n",
    "    for i, (img, target) in enumerate(train_loader):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(img)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % log_interval == 0:\n",
    "            losses.append(loss)\n",
    "            counter.append(\n",
    "                (i * batch_size) + img.size(0) + epoch * len(train_loader.dataset))\n",
    "\n",
    "    return losses, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e915a5-dbba-4898-b9cf-eddd6b9307fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(test_loader, model, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    counter = []\n",
    "    test_loss = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (img, label) in enumerate(test_loader):\n",
    "            img, label = img.to(devitensor.resizce), label.to(device)\n",
    "\n",
    "            output = model(img)\n",
    "            test_loss += torch.nn.functional.cross_entropy(output, label, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1) # Get index of largest log-probability and use that as prediction\n",
    "            num_correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * num_correct / len(test_loader.dataset)\n",
    "    print(accuracy)\n",
    "    return test_loss, num_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3313b28b-684a-4782-857a-3880d7da6969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/3 [4:33:34<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c939200e1ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bcf4691af107>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_loader, model, device, optimizer, log_interval, epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/Users/ash/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "max_epochs=3\n",
    "gamma = 0.95\n",
    "\n",
    "# Recording data\n",
    "log_interval = 100\n",
    "\n",
    "# Instantiate optimizer (model was created in previous cell)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_correct = []\n",
    "for epoch in trange(max_epochs, leave=True, desc='Epochs'):\n",
    "    train_loss, counter = train_one_epoch(train_loader, model, DEVICE, optimizer, log_interval, epoch)\n",
    "    test_loss, num_correct = test_one_epoch(test_loader, model, DEVICE)\n",
    "\n",
    "    # Record results\n",
    "    train_losses.extend(train_loss)\n",
    "    train_counter.extend(counter)\n",
    "    test_losses.append(test_loss)\n",
    "    test_correct.append(num_correct)\n",
    "\n",
    "print(f\"Test accuracy: {test_correct[-1]/len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092edd65-07af-46e9-b1c7-defdad0d409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Draw training loss curve\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(train_counter, train_losses, label='Train loss')\n",
    "plt.plot([i * len(train_loader.dataset) for i in range(1, max_epochs + 1)], \n",
    "         test_losses, label='Test loss', marker='o')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Loss curve', fontsize=24)\n",
    "plt.xlabel('Number of training examples seen', fontsize=16)\n",
    "plt.ylabel('NLL', fontsize=16)\n",
    "plt.legend(loc='upper right', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ba5e6-6c1b-403a-b723-c4516a298814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Show the predictions of the first 20 images of the test dataset\n",
    "iterr = iter(train_loader)\n",
    "images, labels = next(iterr)\n",
    "images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "output = model(images)\n",
    "pred = output.argmax(dim=1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 11))\n",
    "\n",
    "# ------------------\n",
    "# Write your implementation here. Use the code provided in Part 0 to visualize the images.\n",
    "for i in range (0,20):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(images[i].squeeze(), cmap='gray', interpolation='none')\n",
    "    plt.title(f'Label: {labels[i]}', fontsize=14)\n",
    "    plt.axis('off')\n",
    "    print(\"The model predicted a\", int(pred[i]), \"for image\", i+1)\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b151355-a4fc-4152-a5dd-25cad819dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_imgs = torch.Tensor().to(DEVICE)\n",
    "incorrect_preds = [] #torch.IntTensor().to(DEVICE)\n",
    "incorrect_labels = [] #torch.IntTensor().to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test set iterator\n",
    "    it = iter(test_loader)\n",
    "    # Loop over the test set batches until incorrect_imgs.size(0) >= 20\n",
    "    while incorrect_imgs.size(0) < 20:\n",
    "        images, labels = next(it)\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # ------------------\n",
    "        # Write your implementation here.\n",
    "\n",
    "        output = model(images)\n",
    "        pred = output.argmax(dim=1)\n",
    "\n",
    "        # Compare prediction and true labels and append the incorrect predictions\n",
    "        # using `torch.cat`\n",
    "        c = 0\n",
    "        for p in pred:\n",
    "            if p != labels[c]:\n",
    "                incorrect_imgs = torch.cat((incorrect_imgs, images[c]), dim=0)\n",
    "                incorrect_preds.append(p)\n",
    "                incorrect_labels.append(labels[c])\n",
    "            c += 1\n",
    "\n",
    "        # ------------------\n",
    "                \n",
    "# Show the first 20 wrong predictions in test set\n",
    "fig = plt.figure(figsize=(12, 11))\n",
    "for i in range(20):\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(incorrect_imgs[i].squeeze().cpu().numpy(), cmap='gray', interpolation='none')\n",
    "    plt.title(f'Prediction: {incorrect_preds[i].item()}\\nLabel: {incorrect_labels[i].item()}', fontsize=14)\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
